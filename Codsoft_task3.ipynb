{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df92868-66cb-475f-b2ca-c622fba3d596",
   "metadata": {},
   "source": [
    "# Name - Parikshit Sahu\n",
    "## Codsoft Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "207edac1-911e-40ad-9738-2c997b0100e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions data loaded. Previewing first few entries:\n",
      "Image ID: img_1.jpg, Captions: ['A group of people standing on a beach.', 'A person holding a surfboard.', 'Waves crashing on the shore.']\n",
      "Image ID: img_2.jpg, Captions: ['A dog running in a park.', 'A brown dog chasing a ball.', 'Sunset in the background.']\n",
      "Image ID: img_3.jpg, Captions: ['A family having a picnic.', 'People sitting on a blanket.', 'Eating sandwiches and smiling.']\n",
      "Loaded 1 image features\n",
      "Loaded 3 captions data entries\n",
      "Image IDs in image_features: ['Photo']\n",
      "Image IDs in captions_data: ['img_1.jpg', 'img_2.jpg', 'img_3.jpg']\n",
      "Max caption length: 8\n",
      "Vocabulary size: 33\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid captions found. Check the format of your captions.json file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 121\u001b[0m\n\u001b[0;32m    117\u001b[0m     y \u001b[38;5;241m=\u001b[39m pad_sequences(y, maxlen\u001b[38;5;241m=\u001b[39mmax_caption_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X_image), np\u001b[38;5;241m.\u001b[39marray(X_caption), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m--> 121\u001b[0m X_image, X_caption, y \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_caption_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Check that the data is not empty\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_image\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m X_caption\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[38], line 112\u001b[0m, in \u001b[0;36mprepare_training_data\u001b[1;34m(image_features, captions_data, tokenizer, max_caption_length)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Pad input sequences (X_caption) and target sequences (y) to max_caption_length\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_caption) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid captions found. Check the format of your captions.json file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m X_caption \u001b[38;5;241m=\u001b[39m pad_sequences(X_caption, maxlen\u001b[38;5;241m=\u001b[39mmax_caption_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Pad y sequences to max_caption_length - 1, because the last word is missing in the output\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: No valid captions found. Check the format of your captions.json file."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Add, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "dataset_image_dir = r'C:\\Users\\sahup\\dataset_images'  # Replace with your image folder path\n",
    "captions_file = 'captions.json'  # Assuming you have captions.json file in the current directory\n",
    "\n",
    "# Load image features (dummy example with random features for now)\n",
    "def load_image_features(image_dir):\n",
    "    features = {}\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        if img_name.endswith('.jpg') or img_name.endswith('.png'):\n",
    "            image_id = os.path.splitext(img_name)[0]  # Extract file name without extension\n",
    "            features[image_id] = np.random.rand(256)  # Dummy feature vector (replace with actual feature extraction)\n",
    "    return features\n",
    "\n",
    "# Load the captions data\n",
    "def load_captions(captions_path):\n",
    "    with open(captions_path, 'r') as file:\n",
    "        captions_data = json.load(file)\n",
    "    \n",
    "    # Debug: Check the structure of the loaded captions data\n",
    "    print(\"Captions data loaded. Previewing first few entries:\")\n",
    "    for image_id, captions in list(captions_data.items())[:5]:  # Preview first 5 entries\n",
    "        print(f\"Image ID: {image_id}, Captions: {captions}\")\n",
    "        \n",
    "    return captions_data\n",
    "\n",
    "# Load the features and captions\n",
    "image_features = load_image_features(dataset_image_dir)\n",
    "captions_data = load_captions(captions_file)\n",
    "\n",
    "# Debug: Check the keys in image_features and captions_data\n",
    "print(f\"Loaded {len(image_features)} image features\")\n",
    "print(f\"Loaded {len(captions_data)} captions data entries\")\n",
    "\n",
    "# Debug: Show the image features and captions keys to check for matching IDs\n",
    "print(\"Image IDs in image_features:\", list(image_features.keys())[:5])\n",
    "print(\"Image IDs in captions_data:\", list(captions_data.keys())[:5])\n",
    "\n",
    "# Step 2: Tokenize the captions\n",
    "def tokenize_captions(captions_data):\n",
    "    all_captions = []\n",
    "    for img, captions in captions_data.items():\n",
    "        for caption in captions:\n",
    "            if isinstance(caption, str):  # Ensure the caption is a string\n",
    "                all_captions.append(caption)\n",
    "            else:\n",
    "                print(f\"Skipping non-string caption: {caption}\")\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    return tokenizer\n",
    "\n",
    "# Tokenize captions and define max caption length\n",
    "tokenizer = tokenize_captions(captions_data)\n",
    "\n",
    "# Step 2.1: Filter out non-string captions before calculating max_caption_length\n",
    "def get_max_caption_length(captions_data):\n",
    "    # Filter only string captions\n",
    "    all_captions = []\n",
    "    for captions in captions_data.values():\n",
    "        for caption in captions:\n",
    "            if isinstance(caption, str):\n",
    "                all_captions.append(caption)\n",
    "    \n",
    "    # Get the length of the longest caption\n",
    "    return max([len(caption.split()) for caption in all_captions])\n",
    "\n",
    "# Calculate max_caption_length using filtered valid string captions\n",
    "max_caption_length = get_max_caption_length(captions_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "\n",
    "print(f\"Max caption length: {max_caption_length}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Step 3: Prepare the data\n",
    "def prepare_training_data(image_features, captions_data, tokenizer, max_caption_length):\n",
    "    X_image = []\n",
    "    X_caption = []\n",
    "    y = []\n",
    "\n",
    "    for img, captions in captions_data.items():\n",
    "        # Remove the file extension if it exists\n",
    "        image_id = os.path.splitext(img)[0]  # Extract image name without extension\n",
    "        if image_id in image_features:  # Ensure that the image id exists in image_features\n",
    "            for caption in captions:\n",
    "                # Ensure caption is a string before processing\n",
    "                if isinstance(caption, str):\n",
    "                    # Prepare image features (X_image)\n",
    "                    X_image.append(image_features[image_id])\n",
    "\n",
    "                    # Tokenize the caption (X_caption)\n",
    "                    caption_seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                    X_caption.append(caption_seq)\n",
    "\n",
    "                    # The target output is the next word in the sequence (y)\n",
    "                    # We exclude the first token for the target (i.e., predicting the next word)\n",
    "                    y.append(caption_seq[1:])\n",
    "                else:\n",
    "                    print(f\"Skipping non-string caption: {caption}\")\n",
    "\n",
    "    # Pad input sequences (X_caption) and target sequences (y) to max_caption_length\n",
    "    if len(X_caption) == 0:\n",
    "        raise ValueError(\"No valid captions found. Check the format of your captions.json file.\")\n",
    "\n",
    "    X_caption = pad_sequences(X_caption, maxlen=max_caption_length, padding='post')\n",
    "\n",
    "    # Pad y sequences to max_caption_length - 1, because the last word is missing in the output\n",
    "    y = pad_sequences(y, maxlen=max_caption_length - 1, padding='post')\n",
    "\n",
    "    return np.array(X_image), np.array(X_caption), np.array(y)\n",
    "\n",
    "X_image, X_caption, y = prepare_training_data(image_features, captions_data, tokenizer, max_caption_length)\n",
    "\n",
    "# Check that the data is not empty\n",
    "if X_image.size == 0 or X_caption.size == 0 or y.size == 0:\n",
    "    raise ValueError(\"One or more of the input data arrays are empty.\")\n",
    "\n",
    "print(f\"Prepared {len(X_image)} image features, {len(X_caption)} captions, and {len(y)} target sequences.\")\n",
    "\n",
    "# Step 4: Create the image captioning model\n",
    "def create_captioning_model(vocab_size, max_caption_length, embedding_dim=256, lstm_units=512):\n",
    "    # Image feature input (shape: image_feature_size,)\n",
    "    image_input = Input(shape=(256,), name=\"image_input\")\n",
    "    image_embedding = Dense(256, activation='relu')(image_input)  # Ensure same shape as LSTM output\n",
    "    \n",
    "    # Caption input\n",
    "    caption_input = Input(shape=(max_caption_length,), name=\"caption_input\")\n",
    "    caption_embedding = Embedding(vocab_size, embedding_dim, input_length=max_caption_length)(caption_input)\n",
    "    \n",
    "    # LSTM to process the embedded caption\n",
    "    caption_lstm = LSTM(lstm_units, return_sequences=True)(caption_embedding)\n",
    "    \n",
    "    # Adjust the dimension of the caption LSTM output to match image embedding\n",
    "    caption_lstm_projected = Dense(256, activation='relu')(caption_lstm)  # Matching dimensions\n",
    "    \n",
    "    # Combine image features and caption embeddings (both are now 256)\n",
    "    combined = Add()([image_embedding, caption_lstm_projected])  # Shapes are now compatible\n",
    "    \n",
    "    # Further processing\n",
    "    combined = Dense(lstm_units, activation='relu')(combined)\n",
    "    combined = Dropout(0.5)(combined)\n",
    "    \n",
    "    # Change the output layer to predict vocabulary words at each time step\n",
    "    combined = Dense(vocab_size, activation='softmax')(combined)  # Output shape [batch_size, max_caption_length - 1, vocab_size]\n",
    "    \n",
    "    # Create and return the model\n",
    "    model = Model(inputs=[image_input, caption_input], outputs=combined)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "def compile_model(model):\n",
    "    optimizer = Adam()\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Updated for TensorFlow 2.x\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_captioning_model(vocab_size, max_caption_length)\n",
    "\n",
    "# Compile the model\n",
    "model = compile_model(model)\n",
    "\n",
    "# Step 5: Train the model\n",
    "def train_captioning_model(model, X_image, X_caption, y, epochs=10, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the model on the given image features and captions.\n",
    "    \"\"\"\n",
    "    model.fit([X_image, X_caption], y, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "\n",
    "# Train the model\n",
    "train_captioning_model(model, X_image, X_caption, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10ce93-5af0-476d-a90e-9e43fb4191fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
